{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14367278-e288-4703-89fa-fa5b174141d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build-A-Bot\n",
    "\n",
    "#### Quick and easy explanation of building a RL Agent to play Airstriker-Genesis\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e617cd-4255-45fd-a604-4d95888c953c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download all need python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b5b9b-da1f-4821-80e5-f459e47aab95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install box2d-py\n",
    "!pip3 install gym[Box_2D]\n",
    "!pip3 install stable-baselines3\n",
    "!pip3 install gym==0.21.0\n",
    "!pip3 install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850dcad8-fe5a-4cca-904c-00cb4aa27b22",
   "metadata": {},
   "source": [
    "### Import all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7f0ac9-22c8-4919-9bcd-2aebd32050ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3 import DQN\n",
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed77d15-fbeb-4e1c-b707-ddba2d0d9a4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Instantiate the environment\n",
    " - [OpenAI Documentation](https://gym.openai.com/docs/)\n",
    " - [Gym-Retro Documentation](https://retro.readthedocs.io/en/latest/getting_started.html)\n",
    "\n",
    "> Gym started in 2016 as a goal to make open-source environments for RL Agents, similar to how ImageNet was started in 2008 for training Convolutional Neural Networks. Environments, by nature, are complex and uncertain, and the purpose is to provide agents opportunities to learn in a simulated fashion that could represent what they would see in the real world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfbbc98f-0c63-47a1-9984-a92df8487e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb159dc-0f57-4395-a9fa-a3b3fe2031db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's make an agent that takes random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ffdc45-a64e-4a0a-9e46-0bf890dbefc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 13:00:21.771 Python[22839:741631] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/h_/sk5bt9fn1g96qtys09hv8kzc0000gn/T/org.python.python.savedState\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample())\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e914c-ded5-4152-ac75-82aecce3d4b7",
   "metadata": {},
   "source": [
    "### Now... Let's discuss how a machine makes an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a34fb0-ee58-476f-8091-e7fcf8898717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space - Discrete(4)\n",
      "Example of a Possible Random Action - 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action Space - {env.action_space}\")\n",
    "print(f\"Example of a Possible Random Action - {env.action_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e5a16-f750-4e0b-be24-cf9384c5a8aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Why length 4?\n",
    "\n",
    "> 4 possible actions\n",
    "> - Do Nothing\n",
    "> - Fire Main Engine\n",
    "> - Fire Left Engine\n",
    "> - Fire Right Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a990d4-1763-4e5e-8052-6ac7618db6b3",
   "metadata": {},
   "source": [
    "### Now... Let's discuss how an agent sees the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d74939af-c610-46f1-81ba-eaecca7a070b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space - (8,)\n",
      "Example of a Possible Observation - [-0.54296476 -2.2571726  -1.4538776   2.8159966  -1.3927038  -1.6634027\n",
      " -1.1655447  -1.7009467 ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation Space - {env.observation_space.shape}\")\n",
    "print(f\"Example of a Possible Observation - {env.observation_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194e959-7293-44d1-88d2-2d2a7a162511",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Why length 8?\n",
    "> Agent receives 8 inputs as an observation\n",
    "> - Horizontal Position\n",
    "> - Vertical Position\n",
    "> - Horizontal Velocity\n",
    "> - Vertical Velocity\n",
    "> - Angle\n",
    "> - Angular Velocity\n",
    "> - Left Leg Contact\n",
    "> - Right Leg Contact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e4f3fd-37bd-4f7b-bf6c-85c819f367e1",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "#### [Deep Q-Network(DQN)](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)\n",
    "> DQN is a Q-Table based algorithm, which basically is a fancy lookup table. The \"Deep\" part is the addition of a neural network to help it understand more complex environments and observations, such as, atari games. It's what we call an \"Off-Policy\" algorithm, which means, it independently tries to figure out the best policy for a given outcome, regardless of what the agent does. It is also a \"Model-Free\" algorithm, and at a high level, means it takes the route of trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98385e59-c8e3-498f-8244-a5c4c35ef84e",
   "metadata": {},
   "source": [
    "<img src=\"./images/Q-table.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624dfa8-3a59-4a9a-92a2-7509d1f5068b",
   "metadata": {},
   "source": [
    "#### Variables for monitoring model performance through evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73fb7fc1-f846-4626-a957-d4c503bc87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = stable_baselines3.common.monitor.Monitor(env, log_dir )\n",
    "\n",
    "callback = EvalCallback(env,log_path = log_dir, deterministic=True) #For evaluating the performance of the agent periodically and logging the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030ab7c-500c-4b53-97dd-137b4031ea45",
   "metadata": {},
   "source": [
    "#### Specific neural network and model training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1acbb2-7c38-4bb1-9594-1229b3a2cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_layers = [64,64]\n",
    "\n",
    "learning_rate = 0.001 #0.001-0.00001\n",
    "\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=nn_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561709f3-97a7-4ca4-8c01-91b27880c2c7",
   "metadata": {},
   "source": [
    "#### Put it all together in the DQN Model\n",
    "> Don't be scared of all the parameters, DQN comes with defaults and parameters are things you can optionally change during your runs to see if there is better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76bde2e-6519-4cef-9260-da7f65baabf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "#model = DQN(\"MlpPolicy\", env)\n",
    "model = DQN(\"MlpPolicy\", env,policy_kwargs = policy_kwargs,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=1,  #keep the learning simple, learn after each game\n",
    "            buffer_size=1, #how much you want it to learn, set at 1 since batch_size is 1\n",
    "            learning_starts=1, #start learning from the get-go\n",
    "            gamma=0.99, #how much your discount your policy by, between 0-1\n",
    "            tau = 1,  #impacts your learning policy, but by less than gamma\n",
    "            target_update_interval=1, #update the network right away\n",
    "            train_freq=(1,\"step\"), #always train the network at each step\n",
    "            exploration_fraction = 0.5, #how much do you reduce random actions by after each step\n",
    "            gradient_steps = 1, #number of gradient steps\n",
    "            seed = 1, #seed for the pseudo random generators\n",
    "            verbose=1) #output for training logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216d5be-9b96-4d3c-bbc2-433a5f79866c",
   "metadata": {},
   "source": [
    "### Run the model with no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ce8f5d-c21a-421b-ae95-7568aa41e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d32e0-87b9-4899-9e5a-9683010db327",
   "metadata": {},
   "source": [
    "### Train and Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c5304f-e3a4-4f18-96f5-a99d586daba6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -172     |\n",
      "|    exploration_rate | 0.733    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 414      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 421      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.49     |\n",
      "|    n_updates        | 419      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98       |\n",
      "|    ep_rew_mean      | -147     |\n",
      "|    exploration_rate | 0.503    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 407      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 784      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 10.3     |\n",
      "|    n_updates        | 782      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 132      |\n",
      "|    ep_rew_mean      | -142     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 383      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1590     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000388 |\n",
      "|    n_updates        | 1588     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 135      |\n",
      "|    ep_rew_mean      | -209     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 381      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 2161     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.89     |\n",
      "|    n_updates        | 2159     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=3000)\n",
    "model.save(\"./dqn_models/dqn_lunar_3k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b785f-bdaa-419c-827c-965ec4699800",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What is it even learning?\n",
    "\n",
    "<img src=\"./images/Q-table-learning.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c313d2-6a92-4d40-9a1a-b0fedcb5b391",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent with very little training - 3000 time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d1923d5-f7f9-40c5-9055-f1c75c78d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc96ae0-6109-4c63-a8e9-9872d774901a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent with little training - 100k time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207d720c-2163-4ae9-afb3-eda2bd486973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = DQN.load(\"./dqn_models/dqn_lunar_100k\", env=env)\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93baad3-8cf9-40bb-8018-d4bab972d272",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent with medium training - 500k time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f17fb8-1510-4e1c-b04c-fbe788a868a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = DQN.load(\"./dqn_models/dqn_lunar_500k\", env=env)\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f201c6-58ba-4ed3-837b-d4ca7b8c17ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent with larger amounts of training - 1 million time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2db48b43-e9a0-4025-8094-971993041c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = DQN.load(\"./dqn_models/dqn_lunar_1mill\", env=env)\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32dc500-7370-4121-986f-f754a8d6f733",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    " - https://deeplearning.neuromatch.io/projects/ReinforcementLearning/lunar_lander.html\n",
    " - https://www.duckietown.org/\n",
    " - https://www.kaggle.com/competitions/kore-2022-beta\n",
    " - https://www.youtube.com/watch?v=WXuK6gekU1Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
